{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passage = \"\"\"\n",
    "If Cristiano Ronaldo didn't exist, would Lionel Messi have to invent him?\n",
    "\n",
    "The question of how much these two other-worldly players inspire each other is an interesting one,\n",
    "and it's tempting to imagine Messi sitting at home on Tuesday night, watching Ronaldo destroying Atletico, \n",
    "angrily glaring at the TV screen and growling: \"Right, I'll show him!\"\n",
    "\n",
    "As appealing as that picture might be, however, it is probably a false one - from Messi's perspective, at least.\n",
    "\n",
    "He might show it in a different way, but Messi is just as competitive as Ronaldo. Rather than goals and \n",
    "personal glory, however, the Argentine's personal drug is trophies.\n",
    "\n",
    "Ronaldo, it can be said, never looks happy on the field of play unless he's just scored a goal - and even \n",
    "then he's not happy for long, because he just wants to score another one. And that relentless obsession with \n",
    "finding the back of the net has undoubtedly played a major role in his stunning career achievements.\n",
    "\n",
    "Messi, though, is a different animal, shown by the generosity with which he sets up team-mates even if he has \n",
    "a chance to shoot, regularly hands over penalty-taking duties to others and invariably celebrates a goal by turning \n",
    "straight to the player who passed him the ball with an appreciative smile.\n",
    "\n",
    "Rather than being a better player than Ronaldo, Messi's main motivations - according to the people who are close to\n",
    "him - are being the best possible version of Lionel Messi, and winning as many trophies as possible.\n",
    "\n",
    "That theory was supported by Leicester boss Brendan Rodgers when I interviewed him for a book I recently wrote about Messi.\n",
    "\n",
    "Do Messi and Ronaldo inspire each other? \"Maybe subconsciously in some way they've driven each other on,\" said Rodgers.\n",
    "\"But I think both those players inherently have that hunger to be the best players they can be. With the very elite \n",
    "performers, that drive comes from within.\"\n",
    "\n",
    "Messi and Ronaldo ferociously competing with each other for everyone else's acclaim is a nice story for fans to debate \n",
    "and the media to spread, but it's probably not particularly true.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    " \n",
    "sentences = sent_tokenize(passage)    \n",
    "sentences = [expand_contractions(i) for i in sentences]\n",
    "sentences = [re.sub('\\n', '', i) for i in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization is an increasingly popular area within NLP and with the advancements in moderns deep learning, we are consistently seeing newer, more novel approaches. The goal of this article is to compare the results of a few approaches that I found interesting:\n",
    "1. Sentence Scoring based on Word Frequency\n",
    "2. TextRank using Universal Sentence Encoder\n",
    "3. Unsupervised Learning using Skip-Thought Vectors\n",
    "\n",
    "Before moving forward, I wanted to give credit to the outstanding Medium authors/articles who are the foundation for this post and help me learn/implement the Text Summarization techniques below:\n",
    "1. https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65\n",
    "2. https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1\n",
    "3. https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
    "\n",
    "Some of the code snippets they've provided will be shown here as well but I encourage you to read through their posts too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Scoring based on Word Freqency (Python 2.7/3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach we will explore is the simplest of the three. Here we assign weights to each word based on the frequency of the word in the passage. For example, if \"Soccer\" occurs 4 times within the passage, it will have a weight of 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_freq_table(text_string):\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    \n",
    "    words = word_tokenize(text_string)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    freq_table = {}\n",
    "    \n",
    "    for word in words:\n",
    "        #stem word \n",
    "        word = ps.stem(word)\n",
    "        \n",
    "        #remove stopwords\n",
    "        if word in stopwords_list: \n",
    "            continue\n",
    "        elif word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "            \n",
    "    return freq_table\n",
    "\n",
    "freq_table = create_freq_table(\" \".join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the weights assigned to each word above, we will create a score for each sentence. At the end of the day, we will be taking the score of the top `N` for the summary. As you'd imagine, just by leveraging the raw score of each sentence, the length of certain sentences will skew the results. This is why will normalize the scores by dividing by the length of each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentences(sentences, freq_table):\n",
    "    \n",
    "    sentence_value = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_count_in_sentence = len(word_tokenize(sentence))\n",
    "        \n",
    "        for wordValue in freq_table:\n",
    "            \n",
    "            if wordValue.lower() in sentence.lower():                \n",
    "                if sentence in sentence_value:\n",
    "                    sentence_value[sentence] += freq_table[wordValue]\n",
    "                else:\n",
    "                    sentence_value[sentence] = freq_table[wordValue]\n",
    "\n",
    "        sentence_value[sentence] = sentence_value[sentence] // word_count_in_sentence\n",
    "    return sentence_value\n",
    "\n",
    "def find_average_score(sentence_value):\n",
    "    sum_values = 0\n",
    "    \n",
    "    for entry in sentence_value:\n",
    "        sum_values += sentence_value[entry]\n",
    "        \n",
    "    average = int(sum_values/len(sentence_value))\n",
    "    \n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to create the summary, we will take any sentence that has a score that exceeds a threshold. In this case, the threshold will be the average score for for all of the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_summary(sentences, sentence_value, threshold):\n",
    "    sentence_count = 0\n",
    "    \n",
    "    summary = ''\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if sentence in sentence_value and sentence_value[sentence] > threshold:\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "            \n",
    "    return summary \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If Cristiano Ronaldo didn't exist, would Lionel Messi have to invent him? As appealing as that picture might be, however, it is probably a false one - from Messi's perspective, at least. He might show it in a different way, but Messi is just as competitive as Ronaldo. Rather than goals and personal glory, however, the Argentine's personal drug is trophies. Do Messi and Ronaldo inspire each other? \"Maybe subconsciously in some way they've driven each other on,\" said Rodgers. With the very elite performers, that drive comes from within.\"\n"
     ]
    }
   ],
   "source": [
    "#End to End Run\n",
    "freq_table = create_freq_table(\" \".join(sentences))\n",
    "\n",
    "sentence_scores = score_sentences(sentences, freq_table)\n",
    "\n",
    "threshold = find_average_score(sentence_scores)\n",
    "\n",
    "summary = generate_summary(sentences, sentence_scores, 1.0 * threshold)\n",
    "\n",
    "print(re.sub('\\n','',summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Rank using Universal Sentence Embeddings (Python 3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we evaluate the results generated when using universal sentence embeddings and text rank to generate summaries. Before we jump into the code, let's discuss a few concepts that are critical. \n",
    "\n",
    "**Text Rank**\n",
    "This may sound familiar. This is essentially a derivative of the famous PageRank created by the Google cofounders. In PageRank, they generated a matrix that calculaes the probability that a user will move from one page to another. In the case of TextRank, we generate a cosine similarity matrix where we have the similarity of each sentence to each other.\n",
    "\n",
    "A graph is then generated from this cosine similarity matrix and the pagerank algorithm is applied to this graph and scores are then calculated for each sentence. For more information on the Page Rank algorithm, please use the following resource [pagerank link]\n",
    "\n",
    "**Universal Sentence Embeddings**\n",
    "Without going into too much detail, universal sentence embeddings encode word, sentence and paragraph into semantic vectors. They are trained on Deep Averaging Networks. More details can be found here:\n",
    "\n",
    "https://tfhub.dev/google/universal-sentence-encoder/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using C:\\Temp\\tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_0\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_1\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_10\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_11\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_12\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_13\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_14\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_15\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_16\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_2\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_3\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_4\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_5\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_6\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_7\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_8\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_9\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_0/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_1/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_2/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_3/projection\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_3/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SNLI/Classifier/LinearLayer/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SNLI/Classifier/LinearLayer/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SNLI/Classifier/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SNLI/Classifier/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with global_step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Rather than being a better player than Ronaldo, Messi's main motivations - according to the people who are close tohim - are being the best possible version of Lionel Messi, and winning as many trophies as possible. He might show it in a different way, but Messi is just as competitive as Ronaldo. Messi and Ronaldo ferociously competing with each other for everyone else's acclaim is a nice story for fans to debate and the media to spread, but it has / it is probably not particularly true. Do Messi and Ronaldo inspire each other? Ronaldo, it can be said, never looks happy on the field of play unless he has / he is just scored a goal - and even then he has / he is not happy for long, because he just wants to score another one.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "#generate cosine similarity matrix\n",
    "sim_matrix = cosine_similarity(message_embeddings)\n",
    "\n",
    "#create graph and generate scores from pagerank algorithms\n",
    "nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "   \n",
    "num_of_sentences = 5\n",
    "    \n",
    "summary = \" \".join([i[1] for i in ranked_sentences[:num_of_sentences]])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning using Skip Thought Vectors (Python 2.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this, in my opinion, is the newest and most novel approach we've discussed here. The high level approach is as follows:\n",
    "\n",
    "Text Cleaning -> Encoder/Decoder -> K Means Clustering -> Extract Sentences Closest to Cluster Center\n",
    "\n",
    "Again, there are two main concepts I want to discuss before jumping into the solution:\n",
    "\n",
    "**Skip Thought Vectors**\n",
    "\n",
    "Here, we use a encoder/decoder framework to generate feature vectors Taking it from Kushal Chauhan's post, here is how the encoder and decoder layers are defined:\n",
    "1. Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation h(i) for each sentence S(i) in the input. The encoded representation h(i) is obtained by passing final hidden state of the GRU cell (i.e. after it has seen the entire sentence) to multiple dense layers.\n",
    "2. Decoder Network: The decoder network takes this vector representation h(i) as input and tries to generate two sentences - S(i-1) and S(i+1), which could occur before and after the input sentence respectively. Separate decoders are implemented for generation of previous and next sentences, both being GRU-RNNs. The vector representation h(i) acts as the initial hidden state for the GRUs of the decoder networks.\n",
    "\n",
    "Similar to how Word2Vec embeddings are trained by predicting the surrounding words, the Skip Thought Vectors are trained by predicting the sentence at time, t-1 and t+1. As this model is trained, the learned representation (hidden layer) will now place similar sentences closer together which enables higher performance clustering.\n",
    "\n",
    "I encourage you to review the paper on the same subject for more clarity.\n",
    "\n",
    "**K-Means Clustering**\n",
    "\n",
    "Most of you will be familiar with this form of unsupervised learning but I want to elaborate on how it is used and why it is interesting.\n",
    "\n",
    "As we are aware, each cluster will have some center point which, in the vector space, would indicate the point which closely represents the theme of that cluster. With this in mind, when trying to create a summary, we should only need the sentence which is the closest to the center of that cluster. The key here is choosing the correct number of clusters to do a good job of summarizing the content. Kushal's post recommends that we calculate the cluster size by taking 30% of the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters...\n",
      "Compiling encoders...\n",
      "Loading tables...\n",
      "Packing up...\n",
      "38\n",
      "8\n",
      "41\n",
      "13\n",
      "15\n",
      "48\n",
      "17\n",
      "18\n",
      "20\n",
      "22\n",
      "23\n",
      "56\n",
      "25\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "import skipthoughts\n",
    "\n",
    "# You would need to download pre-trained models first\n",
    "model = skipthoughts.load_model()\n",
    "\n",
    "encoder = skipthoughts.Encoder(model)\n",
    "encoded =  encoder.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the skipthoughts dependencies can be found here.\n",
    "As mentioned above, the number of clusters will be the number of sentences that will be included in the summary. For this example, we used a cluster size of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans = kmeans.fit(encoded)\n",
    "\n",
    "n_clusters = int(np.ceil(len(encoded)**0.6))\n",
    "print(n_clusters)\n",
    "\n",
    "avg = []\n",
    "for j in range(n_clusters):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, encoded)\n",
    "ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "summary = ' '.join([sentences[closest[idx]] for idx in ordering])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do Messi and Ronaldo inspire each other? Ronaldo, it can be said, never looks happy on the field of play unless he has / he is just scored a goal - and even then he has / he is not happy for long, because he just wants to score another one. Rather than being a better player than Ronaldo, Messi\\'s main motivations - according to the people who are close tohim - are being the best possible version of Lionel Messi, and winning as many trophies as possible. That theory was supported by Leicester boss Brendan Rodgers when I interviewed him for a book I recently wrote about Messi. With the very elite performers, that drive comes from within.\" \"But I think both those players inherently have that hunger to be the best players they can be.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
